#!/usr/bin/env python
"""
This script divides the list of USAR experiments in a .csv dataset such that they can be processed
in parallel. Since the parameters are not shared across experiments, we can perform inferences in
multiple models in parallel if we have computing resources.

Using the multiprocessing library to spawn an inference function in parallel in a single script
results in a 'AssertionError: daemonic processes are not allowed to have children'. This is due to
the multiple processes creates during inference (one per chain) and the impossibility of creating
children processes from processes spawn with multiprocessing library. Therefore, we do this
parallelization via shell commands. We split the experiments into different processes which are
performed in different TMUX windows of the same TMUX session. If the tmux session does not exist,
this script will create one named as the run ID. If the number of experiments is bigger than the
number of processes they should be split into, some processes will be responsible for performing
inference sequentially in the experiments assigned to them.

Note:
1. This script requires TMUX to be installed in the machine.
2. This script requires Conda to be installed in the machine and have an environment called
    'coordination' with the dependencies listed in requirements.txt installed.
3. Before running this script for the first time, the user has to check if tmux initializes
    conda when a new session or window is created. This is accomplished by copying the conda
    initialization script, typically saved in the shell rc file (e.g., .bashrc), to their shell
    profile file (e.g., .bash_profile).
"""

import argparse
import json
import multiprocessing
import os
from datetime import datetime
from typing import List, Optional

import numpy as np
import pandas as pd

from coordination.common.config import settings
from coordination.common.constants import (DEFAULT_BURN_IN, DEFAULT_NUM_CHAINS,
                                           DEFAULT_NUM_INFERENCE_JOBS,
                                           DEFAULT_NUM_JOBS_PER_INFERENCE,
                                           DEFAULT_NUM_SAMPLES,
                                           DEFAULT_NUTS_INIT_METHOD,
                                           DEFAULT_PROGRESS_SAVING_FREQUENCY,
                                           DEFAULT_SEED, DEFAULT_TARGET_ACCEPT,
                                           DEFAULT_NUM_TIME_POINTS_FOR_PPA,
                                           DEFAULT_PPA_WINDOW)
from coordination.common.tmux import TMUX
from coordination.common.utils import NumpyArrayEncoder
from coordination.model.builder import MODELS, ModelBuilder
from coordination.model.config_bundle.mapper import DataMapper


def infer(out_dir: str,
          experiment_ids: Optional[List[str]],
          evidence_filepath: str,
          model_params_dict_filepath: Optional[str],
          data_mapping_filepath: str,
          model_name: str,
          do_prior: bool,
          do_posterior: bool,
          seed: Optional[int],
          burn_in: int,
          num_samples: int,
          num_chains: int,
          num_jobs_per_inference: int,
          nuts_init_method: str,
          target_accept: float,
          num_inference_jobs: int,
          progress_saving_frequency: int,
          do_ppa: int,
          num_time_points_ppa: int,
          ppa_window: int):
    """
    Runs inference for multiple experiments in parallel.

    @param out_dir: directory where results must be saved.
    @param experiment_ids: an optional comma-separated list of experiment ids to run inference
        for. If undefined, inference will be executed for all the experiments in the dataset.
    @param evidence_filepath: path of the .csv file containing evidence for the model.
    @param model_params_dict_filepath: path of the .json file containing values to replace model
        parameter's default values.
    @param data_mapping_filepath: path to the file containing a mapping between config bundle
        attributes and columns in the evidence dataframe.
    @param model_name: name of the model. One of ["vocalic, "vocalic_semantic"].
    @param do_prior: whether to perform prior predictive checks.
    @param do_posterior: whether to fit the model and perform posterior predictive checks.
    @param seed: random seed for reproducibility.
    @param burn_in: number of samples in the warm-up phase.
    @param num_samples: number of samples from the posterior distribution.
    @param num_chains: number of parallel chains.
    @param num_jobs_per_inference: number of jobs per inference up to the number of chains.
    @param nuts_init_method: initialization method for the NUTS inference algorithm.
    @param target_accept: acceptance probability.
    @param num_inference_jobs: number of jobs to split the different experiments in the data into.
        Each one of these jobs will spawn min(num_chain, num_jobs_per_inference) other processes.
    @param progress_saving_frequency: frequency to save progress in number of samples drawn per
        chain.
    @param do_ppa: whether we want to fit the model multiple times for PPA.
    @param num_time_points_ppa: number of points to sample for PPA.
    @param ppa_window: window size for PPA.
    """

    # Parameters passed to this function relevant for post-analysis. We will save them to a file.
    execution_params = locals().copy()
    del execution_params["out_dir"]
    del execution_params["model_params_dict_filepath"]
    del execution_params["data_mapping_filepath"]

    # Leave 20% of the cores free
    num_cores = int(multiprocessing.cpu_count() * 0.8)
    num_required_cores = min(num_chains, num_jobs_per_inference) * num_inference_jobs
    if num_cores < num_required_cores:
        raise ValueError(f"The machine has {num_cores} cores but {num_required_cores} are "
                         f"required for full parallelization. Reduce the number of inference "
                         f"jobs or the number of jobs per inference and try again.")

    if not do_prior and not do_posterior:
        raise ValueError("No inference to be performed. Set do_prior and/or do_posterior to True.")

    if not os.path.exists(evidence_filepath):
        raise FileNotFoundError(f"Evidence not found in {evidence_filepath}.")

    model_params_dict = None
    if model_params_dict_filepath:
        if not os.path.exists(model_params_dict_filepath):
            raise FileNotFoundError(
                f"Dictionary of parameter values not found in {model_params_dict_filepath}.")

        with open(model_params_dict_filepath) as f:
            model_params_dict = json.load(f)

    if not os.path.exists(data_mapping_filepath):
        raise FileNotFoundError(f"Mapping between evidence and model's parameter not found in "
                                f"{data_mapping_filepath}.")

    run_id = datetime.now().strftime("%Y.%m.%d--%H.%M.%S")
    tmux = TMUX(run_id.replace(".", "_").replace("-", "_"))

    evidence_df = pd.read_csv(evidence_filepath, index_col=0)

    # Define the list of experiments to run inference for based on the experiments in the dataset
    # and optionally the ones provided to the function.
    available_experiment_ids = evidence_df["experiment_id"].unique()
    if experiment_ids:
        valid_experiment_ids = set(available_experiment_ids).intersection(set(experiment_ids))
        if len(valid_experiment_ids) == 0:
            raise ValueError(f"The entered experiments IDs ({experiment_ids}) are not part of the "
                             f"dataset provided.")
        elif len(valid_experiment_ids) < len(experiment_ids):
            invalid_experiment_ids = set(experiment_ids).difference(set(valid_experiment_ids))
            invalid_experiment_ids_str = '\n'.join(invalid_experiment_ids)
            print(f"Inference won't be executed for the following experiment IDs which are not "
                  f"part of the dataset provided: {invalid_experiment_ids_str}")

        experiment_ids = list(valid_experiment_ids)
    else:
        experiment_ids = list(available_experiment_ids)

    experiment_ids.sort()

    config_bundle = ModelBuilder.build_bundle(model_name)
    with open(data_mapping_filepath) as f:
        data_mapping_dict = json.load(f)
        data_mapper = DataMapper(data_mapping_dict)

    config_bundle.update(model_params_dict)
    data_mapper.validate(config_bundle, evidence_df.columns)

    # Update execution params with model config bundle and data mapping.
    execution_params["run_id"] = run_id
    execution_params["model_config_bundle"] = config_bundle.__dict__
    execution_params["data_mapper"] = data_mapping_dict
    execution_params["tmux_session_name"] = tmux.session_name
    execution_params["experiment_ids"] = experiment_ids

    results_folder = f"{out_dir}/{run_id}"
    os.makedirs(results_folder, exist_ok=True)

    # Get absolute path to the bin directory in the project's root folder. We will use it to
    # execute run_inference_sequentially.py from a tmux window.
    bin_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "."))

    # Adjust the number of jobs if the number of experiments is small enough for parallelization.
    num_inference_jobs = min(num_inference_jobs, len(experiment_ids))
    execution_params["num_inference_jobs"] = num_inference_jobs
    experiment_blocks = np.array_split(experiment_ids, num_inference_jobs)

    # Save execution parameters
    print(f"\nInferences will be saved in {results_folder}.")
    with open(f"{results_folder}/execution_params.json", "w") as f:
        json.dump(execution_params, f, indent=4, cls=NumpyArrayEncoder)

    for i, experiments_per_process in enumerate(experiment_blocks):
        # Start a new process in a tmux window for the experiments to be processed.
        experiment_ids = ",".join(experiments_per_process)
        tmux_window_name = experiments_per_process[0]
        if len(experiments_per_process) > 1:
            tmux_window_name += "..."

        # Call the actual inference script (bin/run_inference_sequentially)
        call_python_script_command = (
            f'python3 {bin_dir}/run_inference_sequentially '
            f'--out_dir="{results_folder}" '
            f'--experiment_ids="{experiment_ids}" '
            f'--evidence_filepath="{evidence_filepath}" '
            f'--data_mapping_filepath="{data_mapping_filepath}" '
            f'--model_name="{model_name}" '
            f'--do_prior={do_prior} '
            f'--do_posterior={do_posterior} '
            f'--seed={seed} '
            f'--burn_in={burn_in} '
            f'--num_samples={num_samples} '
            f'--num_chains={num_chains} '
            f'--num_jobs={num_jobs_per_inference} '
            f'--nuts_init_method="{nuts_init_method}" '
            f'--target_accept={target_accept} '
            f'--progress_saving_frequency={progress_saving_frequency} '
            f'--do_ppa={do_ppa} '
            f'--num_time_points_ppa={num_time_points_ppa} '
            f'--ppa_window={ppa_window}'
        )
        if model_params_dict_filepath:
            call_python_script_command += (
                f' --model_params_dict_filepath="{model_params_dict_filepath}"')

        print(f"\nCalling the following command in the TMUX window {tmux_window_name}.")
        print(call_python_script_command)

        tmux.create_window(tmux_window_name)
        tmux.run_command("conda activate coordination")
        tmux.run_command(call_python_script_command)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Runs NUTS inference algorithm for multiple experiments in a .csv file in "
                    "parallel. This script calls the bin/run_inference_sequentially multiple "
                    "times with a series of experiments to be executed in sequence in each job. "
                    ""
                    "This script has an associated run_id (a timestamp) which names a folder that "
                    "is created under out_dir to store results and relevant data. A new TMUX "
                    "session is created named as the run_id and each inference process will run "
                    "in a separate tmux window of that session."
                    ""
                    "Each experiment will spawn min(num_jobs, num_chains) jobs. Thus, the machine "
                    "needs num_inference_jobs * min(num_jobs, num_chains) available cores to "
                    "achieve full parallelization."
    )

    parser.add_argument(
        "--out_dir",
        type=str,
        default=settings.inferences_dir,
        required=False,
        help="Directory where artifacts must be saved.",
    )
    parser.add_argument(
        "--experiment_ids",
        type=str,
        required=False,
        help="A list of experiment ids separated by comma for which we want to perform inference. "
             "If no experiments are informed, inference will be executed for all the experiments "
             "in the dataset.",
    )
    parser.add_argument(
        "--evidence_filepath",
        type=str,
        required=True,
        help="Path of the .csv file containing the evidence data.",
    )
    parser.add_argument(
        "--model_name",
        type=str,
        required=True,
        choices=MODELS,
        help="Model name.",
    )
    parser.add_argument(
        "--model_params_dict_filepath",
        type=str,
        required=False,
        help="Path of the .json file containing parameters of the model to override default "
             "values."
    )
    parser.add_argument(
        "--data_mapping_filepath",
        type=str,
        required=True,
        help="Path of the .json file containing a mapping between column names in the .csv file "
             "containing evidence to the model and config bundle parameters required for "
             "inference."
    )
    parser.add_argument(
        "--do_prior",
        type=int,
        required=False,
        default=1,
        help="Whether to perform prior predictive check. Use the value 0 to deactivate.",
    )
    parser.add_argument(
        "--do_posterior",
        type=int,
        required=False,
        default=1,
        help="Whether to perform posterior inference. Use the value 0 to deactivate.",
    )
    parser.add_argument(
        "--seed",
        type=int,
        required=False,
        default=DEFAULT_SEED,
        help="Random seed to use during inference.",
    )
    parser.add_argument(
        "--burn_in",
        type=int,
        required=False,
        default=DEFAULT_BURN_IN,
        help="Number of samples to discard per chain during posterior inference.",
    )
    parser.add_argument(
        "--num_samples",
        type=int,
        required=False,
        default=DEFAULT_NUM_SAMPLES,
        help="Number of samples to keep per chain during posterior inference.",
    )
    parser.add_argument(
        "--num_chains",
        type=int,
        required=False,
        default=DEFAULT_NUM_CHAINS,
        help="Number of chains to use during posterior inference.",
    )
    parser.add_argument(
        "--num_jobs_per_inference",
        type=int,
        required=False,
        default=DEFAULT_NUM_JOBS_PER_INFERENCE,
        help="Number of jobs to use per inference process. The effective number of jobs is "
             "min(num_jobs, num_chains).",
    )
    parser.add_argument(
        "--nuts_init_method",
        type=str,
        required=False,
        default=DEFAULT_NUTS_INIT_METHOD,
        help="NUTS initialization method.",
    )
    parser.add_argument(
        "--target_accept",
        type=float,
        required=False,
        default=DEFAULT_TARGET_ACCEPT,
        help="Target acceptance probability used to control step size and reduce "
             "divergences during inference.",
    )
    parser.add_argument(
        "--num_inference_jobs",
        type=int,
        required=False,
        default=os.getenv("NUM_JOBS", DEFAULT_NUM_INFERENCE_JOBS),
        help="Number of jobs to split the experiments into.",
    )
    parser.add_argument(
        "--progress_saving_frequency",
        type=int,
        required=False,
        default=DEFAULT_PROGRESS_SAVING_FREQUENCY,
        help="Frequency at which to save the progress in number of samples. For instance, if x, "
             "progress will be saved at every x samples per chain.",
    )
    parser.add_argument(
        "--do_ppa",
        type=int,
        required=False,
        default=0,
        help="Whether to fit the model multiple times for later posterior predictive analysis. If "
             "this option is enabled, this script will fit the model multiple times, one for each "
             "time point sampled for ppa analysis, determined by num_time_points_for_ppa. The "
             "sampled time points determine the number of time steps we want to use to fit the "
             "model. The maximum number of time steps is determined by the number of time steps "
             "in coordination scale minus ppa_window parameter for each experiment. Inference "
             "data objects will be saved in a directory called inference_data and they will get a "
             "suffix indicating indicating the number of time steps used to fit the data (the "
             "sampled time point)."
    )
    parser.add_argument(
        "--num_time_points_ppa",
        type=int,
        required=False,
        default=DEFAULT_NUM_TIME_POINTS_FOR_PPA,
        help="Number of time points to sample if ppa analysis is enabled."
    )
    parser.add_argument(
        "--ppa_window",
        type=int,
        required=False,
        default=DEFAULT_PPA_WINDOW,
        help="Window size for ppa analysis.",
    )

    args = parser.parse_args()

    infer(out_dir=args.out_dir,
          experiment_ids=args.experiment_ids.split(",") if args.experiment_ids else None,
          evidence_filepath=args.evidence_filepath,
          model_params_dict_filepath=args.model_params_dict_filepath,
          data_mapping_filepath=args.data_mapping_filepath,
          model_name=args.model_name,
          do_prior=args.do_prior,
          do_posterior=args.do_posterior,
          seed=args.seed,
          burn_in=args.burn_in,
          num_samples=args.num_samples,
          num_chains=args.num_chains,
          num_jobs_per_inference=args.num_jobs_per_inference,
          nuts_init_method=args.nuts_init_method,
          target_accept=args.target_accept,
          num_inference_jobs=args.num_inference_jobs,
          progress_saving_frequency=args.progress_saving_frequency,
          do_ppa=args.do_ppa,
          num_time_points_ppa=args.num_time_points_ppa,
          ppa_window=args.ppa_window)
